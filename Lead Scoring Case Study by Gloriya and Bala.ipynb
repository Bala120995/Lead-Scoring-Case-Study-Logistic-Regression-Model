{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#  Lead Scoring Case Study by Gloriya and Bala\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Import Necessary Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import visualisation libraries\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary scikit-learn libraries\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import r2_score,roc_auc_score,roc_curve,accuracy_score,confusion_matrix,classification_report\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score,precision_recall_curve,f1_score\n",
    "\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Expand output display to see more rows and columns\n",
    "\n",
    "pd.set_option('display.max_rows',200)\n",
    "\n",
    "pd.set_option('display.max_columns',160)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Inspecting dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "\n",
    "leads_df=pd.read_csv('Leads.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9240, 37)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of rows and columns\n",
    "\n",
    "leads_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prospect ID</th>\n",
       "      <th>Lead Number</th>\n",
       "      <th>Lead Origin</th>\n",
       "      <th>Lead Source</th>\n",
       "      <th>Do Not Email</th>\n",
       "      <th>Do Not Call</th>\n",
       "      <th>Converted</th>\n",
       "      <th>TotalVisits</th>\n",
       "      <th>Total Time Spent on Website</th>\n",
       "      <th>Page Views Per Visit</th>\n",
       "      <th>Last Activity</th>\n",
       "      <th>Country</th>\n",
       "      <th>Specialization</th>\n",
       "      <th>How did you hear about X Education</th>\n",
       "      <th>What is your current occupation</th>\n",
       "      <th>What matters most to you in choosing a course</th>\n",
       "      <th>Search</th>\n",
       "      <th>Magazine</th>\n",
       "      <th>Newspaper Article</th>\n",
       "      <th>X Education Forums</th>\n",
       "      <th>Newspaper</th>\n",
       "      <th>Digital Advertisement</th>\n",
       "      <th>Through Recommendations</th>\n",
       "      <th>Receive More Updates About Our Courses</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Lead Quality</th>\n",
       "      <th>Update me on Supply Chain Content</th>\n",
       "      <th>Get updates on DM Content</th>\n",
       "      <th>Lead Profile</th>\n",
       "      <th>City</th>\n",
       "      <th>Asymmetrique Activity Index</th>\n",
       "      <th>Asymmetrique Profile Index</th>\n",
       "      <th>Asymmetrique Activity Score</th>\n",
       "      <th>Asymmetrique Profile Score</th>\n",
       "      <th>I agree to pay the amount through cheque</th>\n",
       "      <th>A free copy of Mastering The Interview</th>\n",
       "      <th>Last Notable Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7927b2df-8bba-4d29-b9a2-b6e0beafe620</td>\n",
       "      <td>660737</td>\n",
       "      <td>API</td>\n",
       "      <td>Olark Chat</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Page Visited on Website</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Select</td>\n",
       "      <td>Select</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>Better Career Prospects</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Interested in other courses</td>\n",
       "      <td>Low in Relevance</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Select</td>\n",
       "      <td>Select</td>\n",
       "      <td>02.Medium</td>\n",
       "      <td>02.Medium</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Modified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2a272436-5132-4136-86fa-dcc88c88f482</td>\n",
       "      <td>660728</td>\n",
       "      <td>API</td>\n",
       "      <td>Organic Search</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>674</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Email Opened</td>\n",
       "      <td>India</td>\n",
       "      <td>Select</td>\n",
       "      <td>Select</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>Better Career Prospects</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ringing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Select</td>\n",
       "      <td>Select</td>\n",
       "      <td>02.Medium</td>\n",
       "      <td>02.Medium</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Email Opened</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8cc8c611-a219-4f35-ad23-fdfd2656bd8a</td>\n",
       "      <td>660727</td>\n",
       "      <td>Landing Page Submission</td>\n",
       "      <td>Direct Traffic</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1532</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Email Opened</td>\n",
       "      <td>India</td>\n",
       "      <td>Business Administration</td>\n",
       "      <td>Select</td>\n",
       "      <td>Student</td>\n",
       "      <td>Better Career Prospects</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Will revert after reading the email</td>\n",
       "      <td>Might be</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Potential Lead</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>02.Medium</td>\n",
       "      <td>01.High</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Email Opened</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0cc2df48-7cf4-4e39-9de9-19797f9b38cc</td>\n",
       "      <td>660719</td>\n",
       "      <td>Landing Page Submission</td>\n",
       "      <td>Direct Traffic</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>305</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Unreachable</td>\n",
       "      <td>India</td>\n",
       "      <td>Media and Advertising</td>\n",
       "      <td>Word Of Mouth</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>Better Career Prospects</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ringing</td>\n",
       "      <td>Not Sure</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Select</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>02.Medium</td>\n",
       "      <td>01.High</td>\n",
       "      <td>13.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Modified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3256f628-e534-4826-9d63-4a8b88782852</td>\n",
       "      <td>660681</td>\n",
       "      <td>Landing Page Submission</td>\n",
       "      <td>Google</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1428</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Converted to Lead</td>\n",
       "      <td>India</td>\n",
       "      <td>Select</td>\n",
       "      <td>Other</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>Better Career Prospects</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Will revert after reading the email</td>\n",
       "      <td>Might be</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Select</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>02.Medium</td>\n",
       "      <td>01.High</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Modified</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Prospect ID  Lead Number              Lead Origin  \\\n",
       "0  7927b2df-8bba-4d29-b9a2-b6e0beafe620       660737                      API   \n",
       "1  2a272436-5132-4136-86fa-dcc88c88f482       660728                      API   \n",
       "2  8cc8c611-a219-4f35-ad23-fdfd2656bd8a       660727  Landing Page Submission   \n",
       "3  0cc2df48-7cf4-4e39-9de9-19797f9b38cc       660719  Landing Page Submission   \n",
       "4  3256f628-e534-4826-9d63-4a8b88782852       660681  Landing Page Submission   \n",
       "\n",
       "      Lead Source Do Not Email Do Not Call  Converted  TotalVisits  \\\n",
       "0      Olark Chat           No          No          0          0.0   \n",
       "1  Organic Search           No          No          0          5.0   \n",
       "2  Direct Traffic           No          No          1          2.0   \n",
       "3  Direct Traffic           No          No          0          1.0   \n",
       "4          Google           No          No          1          2.0   \n",
       "\n",
       "   Total Time Spent on Website  Page Views Per Visit            Last Activity  \\\n",
       "0                            0                   0.0  Page Visited on Website   \n",
       "1                          674                   2.5             Email Opened   \n",
       "2                         1532                   2.0             Email Opened   \n",
       "3                          305                   1.0              Unreachable   \n",
       "4                         1428                   1.0        Converted to Lead   \n",
       "\n",
       "  Country           Specialization How did you hear about X Education  \\\n",
       "0     NaN                   Select                             Select   \n",
       "1   India                   Select                             Select   \n",
       "2   India  Business Administration                             Select   \n",
       "3   India    Media and Advertising                      Word Of Mouth   \n",
       "4   India                   Select                              Other   \n",
       "\n",
       "  What is your current occupation  \\\n",
       "0                      Unemployed   \n",
       "1                      Unemployed   \n",
       "2                         Student   \n",
       "3                      Unemployed   \n",
       "4                      Unemployed   \n",
       "\n",
       "  What matters most to you in choosing a course Search Magazine  \\\n",
       "0                       Better Career Prospects     No       No   \n",
       "1                       Better Career Prospects     No       No   \n",
       "2                       Better Career Prospects     No       No   \n",
       "3                       Better Career Prospects     No       No   \n",
       "4                       Better Career Prospects     No       No   \n",
       "\n",
       "  Newspaper Article X Education Forums Newspaper Digital Advertisement  \\\n",
       "0                No                 No        No                    No   \n",
       "1                No                 No        No                    No   \n",
       "2                No                 No        No                    No   \n",
       "3                No                 No        No                    No   \n",
       "4                No                 No        No                    No   \n",
       "\n",
       "  Through Recommendations Receive More Updates About Our Courses  \\\n",
       "0                      No                                     No   \n",
       "1                      No                                     No   \n",
       "2                      No                                     No   \n",
       "3                      No                                     No   \n",
       "4                      No                                     No   \n",
       "\n",
       "                                  Tags      Lead Quality  \\\n",
       "0          Interested in other courses  Low in Relevance   \n",
       "1                              Ringing               NaN   \n",
       "2  Will revert after reading the email          Might be   \n",
       "3                              Ringing          Not Sure   \n",
       "4  Will revert after reading the email          Might be   \n",
       "\n",
       "  Update me on Supply Chain Content Get updates on DM Content    Lead Profile  \\\n",
       "0                                No                        No          Select   \n",
       "1                                No                        No          Select   \n",
       "2                                No                        No  Potential Lead   \n",
       "3                                No                        No          Select   \n",
       "4                                No                        No          Select   \n",
       "\n",
       "     City Asymmetrique Activity Index Asymmetrique Profile Index  \\\n",
       "0  Select                   02.Medium                  02.Medium   \n",
       "1  Select                   02.Medium                  02.Medium   \n",
       "2  Mumbai                   02.Medium                    01.High   \n",
       "3  Mumbai                   02.Medium                    01.High   \n",
       "4  Mumbai                   02.Medium                    01.High   \n",
       "\n",
       "   Asymmetrique Activity Score  Asymmetrique Profile Score  \\\n",
       "0                         15.0                        15.0   \n",
       "1                         15.0                        15.0   \n",
       "2                         14.0                        20.0   \n",
       "3                         13.0                        17.0   \n",
       "4                         15.0                        18.0   \n",
       "\n",
       "  I agree to pay the amount through cheque  \\\n",
       "0                                       No   \n",
       "1                                       No   \n",
       "2                                       No   \n",
       "3                                       No   \n",
       "4                                       No   \n",
       "\n",
       "  A free copy of Mastering The Interview Last Notable Activity  \n",
       "0                                     No              Modified  \n",
       "1                                     No          Email Opened  \n",
       "2                                    Yes          Email Opened  \n",
       "3                                     No              Modified  \n",
       "4                                     No              Modified  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the dataframe \n",
    "\n",
    "leads_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9240 entries, 0 to 9239\n",
      "Data columns (total 37 columns):\n",
      " #   Column                                         Non-Null Count  Dtype  \n",
      "---  ------                                         --------------  -----  \n",
      " 0   Prospect ID                                    9240 non-null   object \n",
      " 1   Lead Number                                    9240 non-null   int64  \n",
      " 2   Lead Origin                                    9240 non-null   object \n",
      " 3   Lead Source                                    9204 non-null   object \n",
      " 4   Do Not Email                                   9240 non-null   object \n",
      " 5   Do Not Call                                    9240 non-null   object \n",
      " 6   Converted                                      9240 non-null   int64  \n",
      " 7   TotalVisits                                    9103 non-null   float64\n",
      " 8   Total Time Spent on Website                    9240 non-null   int64  \n",
      " 9   Page Views Per Visit                           9103 non-null   float64\n",
      " 10  Last Activity                                  9137 non-null   object \n",
      " 11  Country                                        6779 non-null   object \n",
      " 12  Specialization                                 7802 non-null   object \n",
      " 13  How did you hear about X Education             7033 non-null   object \n",
      " 14  What is your current occupation                6550 non-null   object \n",
      " 15  What matters most to you in choosing a course  6531 non-null   object \n",
      " 16  Search                                         9240 non-null   object \n",
      " 17  Magazine                                       9240 non-null   object \n",
      " 18  Newspaper Article                              9240 non-null   object \n",
      " 19  X Education Forums                             9240 non-null   object \n",
      " 20  Newspaper                                      9240 non-null   object \n",
      " 21  Digital Advertisement                          9240 non-null   object \n",
      " 22  Through Recommendations                        9240 non-null   object \n",
      " 23  Receive More Updates About Our Courses         9240 non-null   object \n",
      " 24  Tags                                           5887 non-null   object \n",
      " 25  Lead Quality                                   4473 non-null   object \n",
      " 26  Update me on Supply Chain Content              9240 non-null   object \n",
      " 27  Get updates on DM Content                      9240 non-null   object \n",
      " 28  Lead Profile                                   6531 non-null   object \n",
      " 29  City                                           7820 non-null   object \n",
      " 30  Asymmetrique Activity Index                    5022 non-null   object \n",
      " 31  Asymmetrique Profile Index                     5022 non-null   object \n",
      " 32  Asymmetrique Activity Score                    5022 non-null   float64\n",
      " 33  Asymmetrique Profile Score                     5022 non-null   float64\n",
      " 34  I agree to pay the amount through cheque       9240 non-null   object \n",
      " 35  A free copy of Mastering The Interview         9240 non-null   object \n",
      " 36  Last Notable Activity                          9240 non-null   object \n",
      "dtypes: float64(4), int64(3), object(30)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Check the datatype and null value counts of each column\n",
    "\n",
    "leads_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Finding : There are null values in a number of columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Lead Number</th>\n",
       "      <td>9240.0</td>\n",
       "      <td>617188.435606</td>\n",
       "      <td>23405.995698</td>\n",
       "      <td>579533.0</td>\n",
       "      <td>596484.5</td>\n",
       "      <td>615479.0</td>\n",
       "      <td>637387.25</td>\n",
       "      <td>660737.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Converted</th>\n",
       "      <td>9240.0</td>\n",
       "      <td>0.385390</td>\n",
       "      <td>0.486714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotalVisits</th>\n",
       "      <td>9103.0</td>\n",
       "      <td>3.445238</td>\n",
       "      <td>4.854853</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>251.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Time Spent on Website</th>\n",
       "      <td>9240.0</td>\n",
       "      <td>487.698268</td>\n",
       "      <td>548.021466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>936.00</td>\n",
       "      <td>2272.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Page Views Per Visit</th>\n",
       "      <td>9103.0</td>\n",
       "      <td>2.362820</td>\n",
       "      <td>2.161418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asymmetrique Activity Score</th>\n",
       "      <td>5022.0</td>\n",
       "      <td>14.306252</td>\n",
       "      <td>1.386694</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asymmetrique Profile Score</th>\n",
       "      <td>5022.0</td>\n",
       "      <td>16.344883</td>\n",
       "      <td>1.811395</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.00</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              count           mean           std       min  \\\n",
       "Lead Number                  9240.0  617188.435606  23405.995698  579533.0   \n",
       "Converted                    9240.0       0.385390      0.486714       0.0   \n",
       "TotalVisits                  9103.0       3.445238      4.854853       0.0   \n",
       "Total Time Spent on Website  9240.0     487.698268    548.021466       0.0   \n",
       "Page Views Per Visit         9103.0       2.362820      2.161418       0.0   \n",
       "Asymmetrique Activity Score  5022.0      14.306252      1.386694       7.0   \n",
       "Asymmetrique Profile Score   5022.0      16.344883      1.811395      11.0   \n",
       "\n",
       "                                  25%       50%        75%       max  \n",
       "Lead Number                  596484.5  615479.0  637387.25  660737.0  \n",
       "Converted                         0.0       0.0       1.00       1.0  \n",
       "TotalVisits                       1.0       3.0       5.00     251.0  \n",
       "Total Time Spent on Website      12.0     248.0     936.00    2272.0  \n",
       "Page Views Per Visit              1.0       2.0       3.00      55.0  \n",
       "Asymmetrique Activity Score      14.0      14.0      15.00      18.0  \n",
       "Asymmetrique Profile Score       15.0      16.0      18.00      20.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descriptive statistics of numeric variables\n",
    "\n",
    "leads_df.describe().transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Numeric columns  : 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Lead Number', 'Converted', 'TotalVisits',\n",
       "       'Total Time Spent on Website', 'Page Views Per Visit',\n",
       "       'Asymmetrique Activity Score', 'Asymmetrique Profile Score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List Numeric columns\n",
    "\n",
    "num_cols=leads_df.select_dtypes(include=np.number).columns\n",
    "\n",
    "print('Total Numeric columns  :',len(num_cols))\n",
    "      \n",
    "num_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Categorical columns  : 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Prospect ID', 'Lead Origin', 'Lead Source', 'Do Not Email',\n",
       "       'Do Not Call', 'Last Activity', 'Country', 'Specialization',\n",
       "       'How did you hear about X Education', 'What is your current occupation',\n",
       "       'What matters most to you in choosing a course', 'Search', 'Magazine',\n",
       "       'Newspaper Article', 'X Education Forums', 'Newspaper',\n",
       "       'Digital Advertisement', 'Through Recommendations',\n",
       "       'Receive More Updates About Our Courses', 'Tags', 'Lead Quality',\n",
       "       'Update me on Supply Chain Content', 'Get updates on DM Content',\n",
       "       'Lead Profile', 'City', 'Asymmetrique Activity Index',\n",
       "       'Asymmetrique Profile Index',\n",
       "       'I agree to pay the amount through cheque',\n",
       "       'A free copy of Mastering The Interview', 'Last Notable Activity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List Non-numeric columns\n",
    "\n",
    "cat_cols=leads_df.select_dtypes(exclude=np.number).columns\n",
    "\n",
    "print('Total Categorical columns  :',len(cat_cols))\n",
    "\n",
    "cat_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Columns with Missing value\n",
    "\n",
    "sum(leads_df.isnull().sum().values>0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Finding : There are 7 numeric columns and 30 categorical columns . Also there are 17 columns with missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Check for duplicate rows in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "leads_df.duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding : There are no duplicate rows in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Cleaning the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking for columns with unique values. such columns does not add any value to our analysis and we should drop those\n",
    "\n",
    "sum(leads_df.nunique().values==1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Finding :  There are 5 columns which have identical values in all the rows, these columns do not add any value to our analysis, hence we will drop those columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prospect ID                                      9240\n",
       "Lead Number                                      9240\n",
       "Lead Origin                                         5\n",
       "Lead Source                                        21\n",
       "Do Not Email                                        2\n",
       "Do Not Call                                         2\n",
       "Converted                                           2\n",
       "TotalVisits                                        41\n",
       "Total Time Spent on Website                      1731\n",
       "Page Views Per Visit                              114\n",
       "Last Activity                                      17\n",
       "Country                                            38\n",
       "Specialization                                     19\n",
       "How did you hear about X Education                 10\n",
       "What is your current occupation                     6\n",
       "What matters most to you in choosing a course       3\n",
       "Search                                              2\n",
       "Magazine                                            1\n",
       "Newspaper Article                                   2\n",
       "X Education Forums                                  2\n",
       "Newspaper                                           2\n",
       "Digital Advertisement                               2\n",
       "Through Recommendations                             2\n",
       "Receive More Updates About Our Courses              1\n",
       "Tags                                               26\n",
       "Lead Quality                                        5\n",
       "Update me on Supply Chain Content                   1\n",
       "Get updates on DM Content                           1\n",
       "Lead Profile                                        6\n",
       "City                                                7\n",
       "Asymmetrique Activity Index                         3\n",
       "Asymmetrique Profile Index                          3\n",
       "Asymmetrique Activity Score                        12\n",
       "Asymmetrique Profile Score                         10\n",
       "I agree to pay the amount through cheque            1\n",
       "A free copy of Mastering The Interview              2\n",
       "Last Notable Activity                              16\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "unique_cols=leads_df.nunique()\n",
    "\n",
    "unique_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Magazine',\n",
       " 'Receive More Updates About Our Courses',\n",
       " 'Update me on Supply Chain Content',\n",
       " 'Get updates on DM Content',\n",
       " 'I agree to pay the amount through cheque']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of unique value columns\n",
    "\n",
    "unique_cols=list(unique_cols.index[unique_cols.values==1])\n",
    "\n",
    "unique_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unique valued columns\n",
    "\n",
    "leads_df.drop(unique_cols,1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "leads_df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Finding : Now we are left with 32 columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all the values to lower case to avoid ambiguity\n",
    "\n",
    "leads_df = leads_df.applymap(lambda x:x.lower() if type(x) == str else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a number of columns entries with value as 'Select' which is nothing but null values, replace 'Select' with NaN\n",
    "\n",
    "leads_df = leads_df.replace('select', np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Let's inspect each column in details for a better understanding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unemployed              5600\n",
       "Working Professional     706\n",
       "Student                  210\n",
       "Other                     16\n",
       "Housewife                 10\n",
       "Businessman                8\n",
       "Name: What is your current occupation, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leads_df['What is your current occupation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unemployed',\n",
       " 'Working Professional',\n",
       " 'Student',\n",
       " 'Other',\n",
       " 'Housewife',\n",
       " 'Businessman']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(leads_df['What is your current occupation'].value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in leads_df.columns:\n",
    "    print(i,'----------------------------------------------------------------------------------------------------------')\n",
    "    print(leads_df[i].value_counts(dropna=False))\n",
    "    print('-------------------------------------------------------------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "#### Finding : There are a couple of columns with values which does not have enough variability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go ahead and drop the most redundant columns\n",
    "\n",
    "leads_df.drop(['Prospect ID', 'Lead Number'], 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Missing value treatment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look for columns which have missing values\n",
    "\n",
    "sum(leads_df.isnull().mean()>0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### There are 17 out of 30 columns with null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for the percentage  null values in each column\n",
    "\n",
    "null_cols=round(leads_df.isnull().mean()*100,2)\n",
    "\n",
    "null_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will drop all the columns with Null values Greater than 35%\n",
    "\n",
    "drop_cols=null_cols[null_cols.values>35].index.to_list()\n",
    "\n",
    "drop_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "leads_df.drop(drop_cols, axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remaining list of categorical columns\n",
    "\n",
    "cat_cols=leads_df.select_dtypes(exclude=np.number).columns\n",
    "\n",
    "cat_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets inspect the variability of all the categorical columns.\n",
    "\n",
    "# This comes under EDA analysis, however this step is essential here as part of missing value treatment.\n",
    "\n",
    "\n",
    "def autopct(pct): # only show the label when it is > 10%\n",
    "    return ('%.2f' % pct) if pct > 10 else ''\n",
    "\n",
    "fig, ax = plt.subplots(4,4,figsize=(12,12))\n",
    "\n",
    "ax[0, 0].title.set_text(cat_cols[0])\n",
    "ax[0, 0].pie(leads_df[cat_cols[0]].value_counts(dropna=False),startangle=90,autopct=autopct) \n",
    "\n",
    "\n",
    "ax[0, 1].title.set_text(cat_cols[1])\n",
    "ax[0, 1].pie(leads_df[cat_cols[1]].value_counts(dropna=False),startangle=90,autopct=autopct) \n",
    "\n",
    "ax[0, 2].title.set_text(cat_cols[2])\n",
    "ax[0, 2].pie(leads_df[cat_cols[2]].value_counts(dropna=False),startangle=90,autopct=autopct) \n",
    "\n",
    "\n",
    "ax[0, 3].title.set_text(cat_cols[3])\n",
    "ax[0, 3].pie(leads_df[cat_cols[3]].value_counts(dropna=False),startangle=90,autopct=autopct) \n",
    "\n",
    "ax[1, 0].title.set_text(cat_cols[4])\n",
    "ax[1, 0].pie(leads_df[cat_cols[4]].value_counts(dropna=False),startangle=90,autopct=autopct) \n",
    "\n",
    "ax[1, 1].title.set_text(cat_cols[5])\n",
    "ax[1, 1].pie(leads_df[cat_cols[5]].value_counts(dropna=False),startangle=90,autopct=autopct) \n",
    "\n",
    "ax[1, 2].title.set_text(cat_cols[6])\n",
    "ax[1, 2].pie(leads_df[cat_cols[6]].value_counts(),startangle=90,autopct=autopct) \n",
    "\n",
    "ax[1, 3].title.set_text('What matters most to you \\n in choosing a course')\n",
    "ax[1, 3].pie(leads_df[cat_cols[7]].value_counts(),startangle=90,autopct=autopct) \n",
    "\n",
    "ax[2, 0].title.set_text(cat_cols[8])\n",
    "ax[2, 0].pie(leads_df[cat_cols[8]].value_counts(),startangle=90,autopct=autopct) \n",
    "\n",
    "ax[2,1].title.set_text(cat_cols[9])\n",
    "ax[2,1].pie(leads_df[cat_cols[9]].value_counts(),startangle=90,autopct=autopct) \n",
    "\n",
    "ax[2,2].title.set_text(cat_cols[10])\n",
    "ax[2,2].pie(leads_df[cat_cols[10]].value_counts(),startangle=90,autopct=autopct) \n",
    "\n",
    "ax[2,3].title.set_text(cat_cols[11])\n",
    "ax[2,3].pie(leads_df[cat_cols[11]].value_counts(),startangle=90,autopct=autopct) \n",
    "\n",
    "ax[3,0].title.set_text(cat_cols[12])\n",
    "ax[3,0].pie(leads_df[cat_cols[12]].value_counts(),startangle=90,autopct=autopct) \n",
    "\n",
    "\n",
    "ax[3,1].title.set_text(cat_cols[13])\n",
    "ax[3,1].pie(leads_df[cat_cols[13]].value_counts(),startangle=90,autopct=autopct) \n",
    "\n",
    "\n",
    "ax[3,2].title.set_text(cat_cols[14])\n",
    "ax[3,2].pie(leads_df[cat_cols[14]].value_counts(),startangle=90,autopct=autopct) \n",
    "\n",
    "ax[3,3].title.set_text(cat_cols[15])\n",
    "ax[3,3].pie(leads_df[cat_cols[15]].value_counts(),startangle=90,autopct=autopct) \n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As per the above pie charts, we will remove columns which do not have much variability , keeping a cut off at 95%\n",
    "\n",
    "drop_cols = ['Do Not Call','Country','Search','Newspaper Article','What matters most to you in choosing a course','X Education Forums','Newspaper','Digital Advertisement','Through Recommendations']\n",
    "\n",
    "leads_df.drop(drop_cols, axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "null_cols=round(leads_df.isnull().mean()*100,2)\n",
    "\n",
    "null_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Now there are 5 more columns with null values, we will go ahead and impute those with the most appropriate values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Let's inspect the variable 'Lead Source'\n",
    "\n",
    "round(leads_df['Lead Source'].value_counts(dropna=False)/leads_df['Lead Source'].value_counts().sum()*100,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 31% of the categorical variable 'Lead Source' are  'Google ' hence we will go ahead and impute the missing values with this value             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "leads_df['Lead Source']=leads_df['Lead Source'].fillna(leads_df['Lead Source'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine 'TotalVisits'\n",
    "\n",
    "leads_df.TotalVisits.isnull().sum()                                     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "leads_df.TotalVisits.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There seems to be an outlier issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.box(leads_df.TotalVisits,width=400, height=300,color_discrete_sequence=['seagreen'])\n",
    "\n",
    "fig.update_layout(margin=dict(l=20, r=20, t=20, b=20),paper_bgcolor=\"Aquamarine\")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will not cap all the outliers since it will remove a significant number of rows from our datset\n",
    "\n",
    "# Instead we will cap 'TotalVisits' greater than 100\n",
    "\n",
    "leads_df=leads_df[leads_df['TotalVisits']<100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck the box plot\n",
    "\n",
    "fig = px.box(leads_df.TotalVisits,width=400, height=300,color_discrete_sequence=['seagreen'])\n",
    "\n",
    "fig.update_layout(margin=dict(l=20, r=20, t=20, b=20),paper_bgcolor=\"Aquamarine\")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Findings : There are still a few outliers, but the upper range has come down to an acceptable range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will also inspect the target variable\n",
    "\n",
    "leads_df.Converted.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traget Inbalance\n",
    "\n",
    "leads_df.Converted.value_counts()[1]/leads_df.Converted.value_counts().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### We can see there is also inbalance in the target variable 'Converted'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Recheck the missing values\n",
    "\n",
    "null_cols=leads_df.isnull().sum()\n",
    "\n",
    "null_cols[null_cols.values>0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We need impute the missing values of the variable 'What is your current occupation' since occupation of an individual is significant information\n",
    "\n",
    "\n",
    "leads_df['What is your current occupation'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing with mode is  not the right choice here since the missing count is quite high\n",
    "\n",
    "# Rather convert NaN values as 'not provided', instead of dropping the column or imputing with mode\n",
    "\n",
    "leads_df['What is your current occupation'] = leads_df['What is your current occupation'].fillna('not provided')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "leads_df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(leads_df.index))\n",
    "\n",
    "print(round(len(leads_df.index)/9240*100,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### All the missing values has been treated. We are able to maintain 98.5 % rows of the original dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Univariate analysis for Numeric Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter out numeric column for EDA\n",
    "\n",
    "num_cols=leads_df.select_dtypes(include=np.number).columns\n",
    "\n",
    "num_cols= num_cols.drop('Converted') # Target variable not needed\n",
    "\n",
    "num_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Lets analyze numeric variables visually using plotly\n",
    "\n",
    "for y in list(num_cols):\n",
    "    \n",
    "    fig = px.box(leads_df[y],width=400, height=300,color_discrete_sequence=['seagreen'])\n",
    "       \n",
    "    fig.update_layout(margin=dict(l=20, r=20, t=20, b=20),paper_bgcolor=\"aquamarine\")    \n",
    "    \n",
    "    fig.show()   \n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "for y in list(num_cols):\n",
    "    \n",
    "    fig = px.histogram(leads_df, x=y,color_discrete_sequence=['seagreen'], width=900, height=500)\n",
    "    \n",
    "    fig.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Finding : There seems to be outliers for the column ''Page Views Per Visit''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[18,4])\n",
    "    \n",
    "plt.subplot(1,3,1)\n",
    "        \n",
    "sns.boxplot(data=leads_df,x=uni_numeric[0],palette=\"Accent\",orient='v')\n",
    "    \n",
    "plt.title('Client '+uni_numeric[0],fontsize=16,loc='right')\n",
    "plt.subplot(1,3,2)\n",
    "        \n",
    "sns.boxplot(data=leads_df,x=uni_numeric[1],palette=\"Accent\",orient='v')\n",
    "    \n",
    "plt.title('Client '+uni_numeric[1],fontsize=16,loc='right')\n",
    "plt.subplot(1,3,3)\n",
    "        \n",
    "sns.boxplot(data=leads_df,x=uni_numeric[2],palette=\"Accent\",orient='h')\n",
    "    \n",
    "plt.title('Client '+uni_numeric[2],fontsize=16,loc='right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "leads_df['Page Views Per Visit'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will treat the outliers by capping 'Page Views Per Visit' greater than 15\n",
    "\n",
    "leads_df=leads_df[leads_df['Page Views Per Visit']<15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize = (10,5))    \n",
    "    \n",
    "fig = px.box(y='Page Views Per Visit',data_frame=leads_df, width=400, height=300,color_discrete_sequence=['seagreen'])\n",
    "    \n",
    "fig.update_layout(margin=dict(l=20, r=20, t=20, b=20),paper_bgcolor=\"aquamarine\")    \n",
    "    \n",
    "fig.show()   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Uni variate Analysis - Categorical Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Filter out categorical  column for EDA\n",
    "\n",
    "cat_cols=leads_df.select_dtypes(exclude=np.number).columns\n",
    "\n",
    "for col in cat_cols:\n",
    "    \n",
    "    plt.figure(figsize = (10,5))\n",
    "    sns.countplot(x=col,data= leads_df,palette='gist_ncar').tick_params(axis='x', rotation = 90)\n",
    "    plt.title(col+'\\n')\n",
    "    plt.show()\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Finding : 'Last Notable Activity' and 'Last Activity' exhibits similar pattern of values.\n",
    "\n",
    "#### we will further analyze it in detail with heatmap analysis to find out any possible correlations and will take necessary actions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Trying the same plot with Plotly only for learning purpose\n",
    "\n",
    "cat_cols=leads_df.select_dtypes(exclude=np.number).columns\n",
    "\n",
    "for col in cat_cols:\n",
    "    \n",
    "    fig = px.bar(leads_df[col].value_counts(),height=500,width=700,color=leads_df[col].value_counts(),color_continuous_scale='armyrose',title=col)\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Bivariate Analysis Categorical Variable w.r.t Target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for col in cat_cols:\n",
    "    \n",
    "    plt.figure(figsize = (15,5))\n",
    "    sns.countplot(x=col, hue='Converted', data= leads_df,palette='Accent').tick_params(axis='x', rotation = 90)\n",
    "    plt.title(col)\n",
    "    plt.show()\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Finding : As per the bivariate analysis as well, 'Last Notable Activity' and 'Last Activity' exhibits similar pattern of distribution with the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Multivariate Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the correlation among numeric variables\n",
    "\n",
    "leads_df.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for numeric variables \n",
    "\n",
    "sns.heatmap(leads_df.corr(),annot=True, cmap='GnBu')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "1. 'TotalVisits' and 'Page Views Per Visit' have a correlation of .65, indicates possible multucollineariy, hence we will drop one of the variable\n",
    "\n",
    "2. 'Total Time Spent on Website' and target variable 'Converted' have a correlation of 0.35, indicates that 'Total Time Spent on Website' could a possible predictor of successful leads.\n",
    "\n",
    "3. 'Total Time Spent on Website' and 'Page Views Per Visit' have a correlation coeffcient of .34\n",
    "\n",
    "#### We can further analyse these variables with VIF treatment as part of model building.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dropping 'Page Views Per Visit' since this variable is least correlated with the target.\n",
    "\n",
    "leads_df.drop('Page Views Per Visit',1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Trying the same scatter plot with Plotly only for learning purpose\n",
    "\n",
    "fig = px.imshow(leads_df.corr(),width=800, height=500,title ='Correlation Heatmap',color_continuous_scale='armyrose')\n",
    "\n",
    "#fig.update_layout(margin=dict(l=50, r=20, t=20, b=20),paper_bgcolor=\"Aquamarine\")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's analyze the pairplot for numeric variables\n",
    "\n",
    "sns.pairplot(leads_df,diag_kind='kde',hue='Converted')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Findings : \n",
    "\n",
    "1. Only the variables 'TotalVisits' and 'Page Views Per Visit' shows a positive correlation as per the scatter plot.\n",
    "\n",
    "2. Also it shows that even though the page views are compararively less but if the person visit the website frequently, that indicates a hot lead.\n",
    "\n",
    "3. If a lead spends more time on the web site it is an indication of a possible lead.\n",
    "\n",
    "4. More time spend on the website as well more pages viewed per visit is a good indication of successful conversion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Craete a dataframe of only numeric variables\n",
    "\n",
    "num_df=leads_df.select_dtypes(include=np.number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Trying the same scatter plot with Plotly only for learning purpose\n",
    "\n",
    "fig = px.scatter_matrix(data_frame=num_df,color_continuous_scale='armyrose',color ='Converted')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once again check null values and confirm\n",
    "\n",
    "leads_df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Dummy Variable Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "leads_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#There is just one variable with Yes/No values\n",
    "\n",
    "# Convert Yes or No values to 1 and 0 respectively\n",
    "\n",
    "\n",
    "leads_df['A free copy of Mastering The Interview']=leads_df['A free copy of Mastering The Interview'].replace(['yes','no'],[1,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out remaining categorical columns\n",
    "\n",
    "cat_cols=list(leads_df.select_dtypes(exclude=np.number).columns)\n",
    "\n",
    "cat_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dummies for the categorical variables\n",
    "\n",
    "dummies=pd.get_dummies(leads_df[cat_cols],drop_first = True) \n",
    "\n",
    "leads_df=pd.concat([leads_df,dummies],axis=1)\n",
    "\n",
    "leads_df.drop(cat_cols,axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create the correlation heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now all the variables are being converted to numeric\n",
    "\n",
    "leads_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "\n",
    "sns.heatmap(leads_df.corr(),cmap='Greens')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As per the heatmap, variables which are higly correlated with Target are:\n",
    "\n",
    "1. 'Last Notable Activity_sms sent'\n",
    "\n",
    "\n",
    "2. 'Last Notable Activity_modified'\n",
    "\n",
    "\n",
    "3. 'What is your current occupation_working professional'\n",
    "\n",
    "\n",
    "4.  'What is your current occupation_not provided'\n",
    "\n",
    "\n",
    "5.  'Last Activity_sms sent'\n",
    "\n",
    "\n",
    "6.  Last Activity_olark chat conversation'\n",
    "\n",
    "\n",
    "7. 'Last Activity_page visited on website'\n",
    "\n",
    "\n",
    "8. Total Time Spent on Website'\n",
    "\n",
    "\n",
    "9. 'Lead Origin_lead add form'\n",
    "\n",
    "\n",
    "10. 'Lead Source_olark chat'\n",
    "\n",
    "\n",
    "11. 'Lead Source_direct traffic'\n",
    "\n",
    "       \n",
    "12. 'Lead Source_reference'\n",
    "\n",
    "\n",
    "13. 'Last Activity_converted to lead'\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The below variables are higly correlated to each other and hence we will go ahead and drop one of the variable\n",
    "\n",
    "\n",
    "\n",
    "1. 'What is your current occupation_not provided' Vs 'What is your current occupation_unemployed'\n",
    "\n",
    "\n",
    "2. 'Last Activity_email opened' Vs 'Last Activity_sms sent' \n",
    "\n",
    "\n",
    "3. 'Lead Source_reference' Vs 'Lead Origin_lead add form'\n",
    "\n",
    "\n",
    "4. 'Lead Source_facebook' Vs 'Lead Origin_lead import'\n",
    "\n",
    "\n",
    "5.   'Last Notable Activity_olark chat conversation' Vs 'Last Activity_olark chat conversation'\n",
    "\n",
    "\n",
    "6.  'Last Activity_sms sent' Vs 'Last Notable Activity_sms sent'\n",
    "\n",
    "\n",
    "7.\t'Last Notable Activity_had a phone conversation' Vs 'Last Activity_had a phone conversation'\n",
    "\n",
    "\n",
    "8.\t'Last Activity_unreachable' Vs 'Last Notable Activity_unreachable'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the most correlated dummy variables\n",
    "\n",
    "leads_df.drop(['What is your current occupation_unemployed','Last Activity_email opened','Lead Source_reference','Lead Source_facebook','Last Notable Activity_olark chat conversation','Last Activity_sms sent','Last Notable Activity_had a phone conversation','Last Activity_unreachable'],1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck the heatmap\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "\n",
    "sns.heatmap(leads_df.corr(),cmap='Greens')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "# Trying the same scatter plot with Plotly only for learning purpose\n",
    "\n",
    "fig = px.imshow(leads_df.corr(),width=1000, height=1000,title ='Correlation Heatmap',color_continuous_scale='armyrose')\n",
    "\n",
    "fig.update_layout(margin=dict(l=50, r=20, t=20, b=20),paper_bgcolor=\"gainsboro\")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Predictors and target variables for model building\n",
    "\n",
    "y=leads_df[['Converted']]\n",
    "\n",
    "X=leads_df.drop('Converted',1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the MinMaxScaler to scale numeric variables\n",
    "\n",
    "scaler=MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Page Views Per Visit' was dropped in an earlier step, hence will drop the same from our list of numeric variables\n",
    "\n",
    "num_cols=num_cols.drop('Page Views Per Visit')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the training dataset\n",
    "\n",
    "X_train[num_cols]=scaler.fit_transform(X_train[num_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import statsmodel library\n",
    "\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Model Building and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Logistic regression model - GLM(Generalized Linear Model)\n",
    "\n",
    "logm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\n",
    "\n",
    "res=logm1.fit()\n",
    "\n",
    "res.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Recursive Feature Elimination for initial screening of variables\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "logreg = LogisticRegression() # Create a Logistic regression class object as an input for RFE\n",
    "\n",
    "logreg.fit(X_train,y_train) # Fit the model with the training data\n",
    "\n",
    "rfe = RFE(logreg, 15)             # running RFE with 15 variables as output\n",
    "\n",
    "rfe = rfe.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a datframe to view the RFE output with the selected variables and their respective ranking\n",
    "\n",
    "rfe_leads_df = pd.DataFrame({'Predictor': X_train.columns, 'Select Status': rfe.support_, 'Ranking': rfe.ranking_})\n",
    "\n",
    "rfe_leads_df.sort_values(by='Ranking')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables selected by RFE\n",
    "\n",
    "rfe_cols = X_train.columns[rfe.support_]\n",
    "\n",
    "rfe_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply GLM model to the selected features\n",
    "\n",
    "X_train_sm = sm.add_constant(X_train[rfe_cols])\n",
    "\n",
    "logm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "\n",
    "res=logm2.fit()\n",
    "\n",
    "res.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate VIF for multicollinearity among variables\n",
    "\n",
    "def fetch_vif_df(local_df):\n",
    "    \n",
    "    vif_df = pd.DataFrame()\n",
    "    \n",
    "    vif_df['Features'] = local_df.columns\n",
    "    \n",
    "    vif_df['VIF'] = [variance_inflation_factor(local_df.values, i) for i in range(local_df.shape[1])]\n",
    "    \n",
    "    vif_df['VIF'] = round(vif_df['VIF'], 2)\n",
    "    \n",
    "    vif_df = vif_df.sort_values(by='VIF', ascending=False)\n",
    "    \n",
    "    vif_df = vif_df.reset_index(drop=True)\n",
    "    \n",
    "    return vif_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# VIF for Feature elimination\n",
    "\n",
    "fetch_vif_df(X_train[rfe_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the target variable for the training data\n",
    "\n",
    "y_train_pred = res.predict(sm.add_constant(X_train[rfe_cols]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with the actual Conversion and the predicted probabilities\n",
    "\n",
    "y_train_pred_df = pd.DataFrame({'Original_Conver':y_train.Converted, 'Conver_Prob':y_train_pred,'ID': y_train.index})\n",
    "\n",
    "y_train_pred_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for the predictions\n",
    "\n",
    "y_train_pred_df['predicted'] = y_train_pred_df.Conver_Prob.map(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "y_train_pred_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix \n",
    "\n",
    "confusion = metrics.confusion_matrix(y_train_pred_df.Original_Conver, y_train_pred_df.predicted )\n",
    "\n",
    "print(confusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the model accuracy\n",
    "\n",
    "print(\"Accuracy of the model is :  \",round(metrics.accuracy_score(y_train_pred_df.Original_Conver, y_train_pred_df.predicted),2)*100, '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'What is your current occupation_housewife' is having a high p-value - makes it an insignificant variable\n",
    "\n",
    "# hence will drop this variable\n",
    "\n",
    "rfe_cols=rfe_cols.drop('What is your current occupation_housewife')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun the model with new set of variables\n",
    "\n",
    "X_train_sm = sm.add_constant(X_train[rfe_cols])\n",
    "\n",
    "logm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "\n",
    "res=logm3.fit()\n",
    "\n",
    "res.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF for Feature elimination\n",
    "\n",
    "fetch_vif_df(X_train[rfe_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the target with the current model\n",
    "\n",
    "y_train_pred = res.predict(sm.add_constant(X_train[rfe_cols]))\n",
    "\n",
    "y_train_pred_df['Conver_Prob']=y_train_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the 'predicted' column with the new predictions\n",
    "\n",
    "y_train_pred_df['predicted'] = y_train_pred_df.Conver_Prob.map(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "y_train_pred_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix \n",
    "\n",
    "confusion = metrics.confusion_matrix(y_train_pred_df.Original_Conver, y_train_pred_df.predicted )\n",
    "\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's check the model accuracy\n",
    "\n",
    "print(\"Accuracy of the model is :  \",round(metrics.accuracy_score(y_train_pred_df.Original_Conver, y_train_pred_df.predicted),2)*100, '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Last Activity_email bounced' got a comparatively high p-value, hence dropping\n",
    "\n",
    "rfe_cols=rfe_cols.drop('Last Activity_email bounced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Rerun the model with new set of variables\n",
    "\n",
    "X_train_sm = sm.add_constant(X_train[rfe_cols])\n",
    "\n",
    "logm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "\n",
    "res=logm4.fit()\n",
    "\n",
    "res.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF for Feature elimination\n",
    "\n",
    "fetch_vif_df(X_train[rfe_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the target with the current model\n",
    "\n",
    "y_train_pred = res.predict(sm.add_constant(X_train[rfe_cols]))\n",
    "\n",
    "y_train_pred_df['Conver_Prob']=y_train_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the 'predicted' column with the new predictions\n",
    "\n",
    "y_train_pred_df['predicted'] = y_train_pred_df.Conver_Prob.map(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "y_train_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix \n",
    "\n",
    "confusion = metrics.confusion_matrix(y_train_pred_df.Original_Conver, y_train_pred_df.predicted )\n",
    "\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's check the model accuracy\n",
    "\n",
    "print(\"Accuracy of the model is :  \",round(metrics.accuracy_score(y_train_pred_df.Original_Conver, y_train_pred_df.predicted),2)*100, '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Lead Source_welingak website' got a comparatively high p-value, hence dropping\n",
    "\n",
    "rfe_cols=rfe_cols.drop('Lead Source_welingak website')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun the model with new set of variables\n",
    "\n",
    "X_train_sm = sm.add_constant(X_train[rfe_cols])\n",
    "\n",
    "logm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "\n",
    "res=logm5.fit()\n",
    "\n",
    "res.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# VIF for Feature elimination\n",
    "\n",
    "fetch_vif_df(X_train[rfe_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the target with the current model\n",
    "\n",
    "y_train_pred = res.predict(sm.add_constant(X_train[rfe_cols]))\n",
    "\n",
    "y_train_pred_df['Conver_Prob']=y_train_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the 'predicted' column with the new predictions\n",
    "\n",
    "y_train_pred_df['predicted'] = y_train_pred_df.Conver_Prob.map(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "y_train_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix \n",
    "\n",
    "confusion = metrics.confusion_matrix(y_train_pred_df.Original_Conver, y_train_pred_df.predicted )\n",
    "\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's check the overall accuracy\n",
    "\n",
    "accuracy = round(metrics.accuracy_score(y_train_pred_df.Original_Conver, y_train_pred_df.predicted),2)*100\n",
    "\n",
    "print(\"Accuracy of the model is :  \",accuracy,'%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rfe_cols=rfe_cols.drop('Lead Source_welingak website')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun the model with new set of variables\n",
    "\n",
    "X_train_sm = sm.add_constant(X_train[rfe_cols])\n",
    "\n",
    "logm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "\n",
    "res=logm5.fit()\n",
    "\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# VIF for Feature elimination\n",
    "\n",
    "fetch_vif_df(X_train[rfe_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The remaining 12 columns are statistically significant, hence this will be our final model\n",
    "\n",
    "All P-values negligibly small(less than 0.05) and we can see that all features are having vif values less than 5, hence there is no multicollinearity issue in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "confusion = confusion_matrix(y_train_pred_df.Original_Conver, y_train_pred_df.predicted)\n",
    "\n",
    "confusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's calculate the sensitivity\n",
    "\n",
    "sensitivity= round(TP / float(TP+FN)*100,2)\n",
    "\n",
    "print(\"Sensitivity of the model is :\", sensitivity,'%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Sensitivity of 70% is comparatively smaller for this analysis, which is not we are aiming for.\n",
    "\n",
    "#### However we will try to improve it with an optimum cut off.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let us calculate specificity\n",
    "\n",
    "specificity= round(TN / float(TN+FP)*100,2)\n",
    "\n",
    "print(\"Specificity of the model is :\", specificity,'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = round(precision_score(y_train_pred_df.Original_Conver, y_train_pred_df.predicted)*100,2)\n",
    "\n",
    "print(\"Precision of the model is :\", precision,'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recall=round(recall_score(y_train_pred_df.Original_Conver, y_train_pred_df.predicted)*100,2)\n",
    "\n",
    "print(\"Recall of the model is :\", recall,'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to draw Receiver operating characteristic Curve - True positive rate (TPR) Vs false positive rate (FPR)\n",
    "\n",
    "def draw_roc( actual, probs ):\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve( actual, probs,\n",
    "                                              drop_intermediate = False )\n",
    "    auc_score = roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(6,6))\n",
    "    \n",
    "    plt.plot(fpr, tpr,'g-', label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--',c='r')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    \n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic Curve \\n',fontdict={'fontsize': 15})\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define false positive rate (FPR), true positive rate (TPR) and Threshold values using roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve( y_train_pred_df.Original_Conver, y_train_pred_df.predicted, drop_intermediate = False )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Plot ROC curve and find the AOC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "draw_roc(y_train_pred_df.Original_Conver, y_train_pred_df.Conver_Prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points to be concluded from above roc curve\n",
    "\n",
    "1. The curve is closer to the top left corner  of the border and this is a measure of good accuracy.\n",
    "\n",
    "2. Here the area under the curve is 89 % of the total area.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create columns with different probability cutoffs \n",
    "\n",
    "numbers = [float(x)/10 for x in range(10)]\n",
    "\n",
    "for i in numbers:\n",
    "    \n",
    "    y_train_pred_df[i]= y_train_pred_df.Conver_Prob.map(lambda x: 1 if x > i else 0)\n",
    "    \n",
    "y_train_pred_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n",
    "\n",
    "cutoff_leads_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n",
    "\n",
    "# TP = confusion[1,1] # true positive \n",
    "# TN = confusion[0,0] # true negatives\n",
    "# FP = confusion[0,1] # false positives\n",
    "# FN = confusion[1,0] # false negatives\n",
    "\n",
    "num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "for i in num:\n",
    "    \n",
    "    cm1 = confusion_matrix(y_train_pred_df.Original_Conver, y_train_pred_df[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "    \n",
    "    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    cutoff_leads_df.loc[i] =[ i ,accuracy,sensi,speci]\n",
    "    \n",
    "print(cutoff_leads_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's draw Accuracy Vs Sensitivity Vs Specificity to see the optimal cut off\n",
    "\n",
    "cutoff_leads_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### From the curve above, it's around 0.32 we have an optimal value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the new threshold value of .32\n",
    "\n",
    "y_train_pred_df['final_predicted'] = y_train_pred_df.Conver_Prob.map( lambda x: 1 if x > .34 else 0)\n",
    "\n",
    "y_train_pred_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's check the accuracy with the revised threshold\n",
    "\n",
    "train_accuracy=round(metrics.accuracy_score(y_train_pred_df.Original_Conver, y_train_pred_df.final_predicted)*100,2)\n",
    "\n",
    "print(\"Accuracy :\", train_accuracy,'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New confusion matrix\n",
    "\n",
    "confusion2 = metrics.confusion_matrix(y_train_pred_df.Original_Conver, y_train_pred_df.final_predicted )\n",
    "confusion2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TP = confusion2[1,1] # true positive \n",
    "TN = confusion2[0,0] # true negatives\n",
    "FP = confusion2[0,1] # false positives\n",
    "FN = confusion2[1,0] # false negatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's check the evaluation scores of the training data\n",
    "\n",
    "train_sensitivity = round(TP / float(TP+FN)*100,2)\n",
    "\n",
    "train_specificity= round(TN / float(TN+FP)*100,2)\n",
    "\n",
    "train_precision=round(precision_score(y_train_pred_df.Original_Conver, y_train_pred_df.final_predicted)*100,2)\n",
    "\n",
    "train_recall= round(recall_score(y_train_pred_df.Original_Conver, y_train_pred_df.final_predicted)*100,2)\n",
    "\n",
    "train_f1_Score=round(f1_score(y_train_pred_df.Original_Conver, y_train_pred_df.final_predicted)*100,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('SCORES FOR THE TRAINING DATA SET \\n')  \n",
    "\n",
    "print( 'Accuracy  : ', train_accuracy,'%\\n')\n",
    "\n",
    "print( 'Sensitivity : ', train_sensitivity,'%\\n')\n",
    "\n",
    "print( 'Specificity : ', train_specificity,'%\\n')\n",
    "\n",
    "print( 'Precision : ', train_precision,'%\\n')\n",
    "\n",
    "print( 'Recall : ', train_recall,'%\\n')\n",
    "\n",
    "print('F1 score : ',train_f1_Score,'%\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Let's draw the Precision Vs Recall \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "p, r, thresholds = precision_recall_curve(y_train_pred_df.Original_Conver, y_train_pred_df.Conver_Prob)\n",
    "\n",
    "plt.plot(thresholds, p[:-1], \"g-\")\n",
    "plt.plot(thresholds, r[:-1], \"r-\")\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Looks like Precision-Recall cut ooff value is slighly higher than the Sensitivity-Specificity cut-off\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 11: Making predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling numeric variables for the validation dataset\n",
    "\n",
    "X_test[num_cols]=scaler.transform(X_test[num_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test[rfe_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prediction on the test dataset\n",
    "\n",
    "y_test_pred= res.predict(sm.add_constant(X_test[rfe_cols]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Lead Scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We will add a new column as Lead_Score which assigns a scoring for each lead, higher the score means higher chance of conversion\n",
    "\n",
    "\n",
    "y_test_pred_df = pd.DataFrame({'Original_Conver':y_test.Converted, 'Conver_Prob':y_test_pred,'Lead_Score':y_test_pred*100,'ID': y_test.index})\n",
    "\n",
    "y_test_pred_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with the actual Conversion and the predicted probabilities for the test data\n",
    "\n",
    "y_test_pred_df['predicted'] = y_test_pred_df.Conver_Prob.map(lambda x: 1 if x > 0.34 else 0)\n",
    "\n",
    "y_test_pred_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for the test data\n",
    "\n",
    "test_confusion = confusion_matrix(y_test_pred_df.Original_Conver, y_test_pred_df.predicted)\n",
    "\n",
    "test_confusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TP = test_confusion[1,1] # true positive \n",
    "TN = test_confusion[0,0] # true negatives\n",
    "FP = test_confusion[0,1] # false positives\n",
    "FN = test_confusion[1,0] # false negatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Let's check the evaluation scores of the validation data\n",
    "\n",
    "test_accuracy = round(accuracy_score(y_test_pred_df.Original_Conver, y_test_pred_df.predicted)*100,2)\n",
    "\n",
    "test_sensitivity = round(TP / float(TP+FN)*100,2)\n",
    "\n",
    "test_specificity= round(TN / float(TN+FP)*100,2)\n",
    "\n",
    "test_precision=round(precision_score(y_test_pred_df.Original_Conver, y_test_pred_df.predicted)*100,2)\n",
    "\n",
    "test_recall= round(recall_score(y_test_pred_df.Original_Conver, y_test_pred_df.predicted)*100,2)\n",
    "\n",
    "test_f1_Score=round(f1_score(y_test_pred_df.Original_Conver, y_test_pred_df.predicted)*100,2)\n",
    "\n",
    "#F1 Score = 2*((precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('SCORES FOR THE VALIDATION DATATSET \\n')      \n",
    "\n",
    "print( 'Accuracy  : ', test_accuracy,'%\\n')\n",
    "\n",
    "print( 'Sensitivity : ', test_sensitivity,'%\\n')\n",
    "\n",
    "print( 'Specificity : ', test_specificity,'%\\n')\n",
    "\n",
    "print( 'Precision : ', test_precision,'%\\n')\n",
    "\n",
    "print( 'Recall : ', test_recall,'%\\n')\n",
    "\n",
    "print('F1 score : ',test_f1_Score,'%\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion:\n",
    "\n",
    "Most significant features those can contribute towards better conversion rate are :\n",
    "\n",
    "1. Total Time Spent on Website\n",
    "2. Lead Origin_lead add form\n",
    "3. What is your current occupation_working professional\n",
    "4. Last Activity_had a phone conversation\n",
    "5. TotalVisits\n",
    "\n",
    "X Education can make use of this model to target the potential candidates to screen those who have a higher probability of enrolling to their programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
